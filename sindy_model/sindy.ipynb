{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1EWx13AGiaPq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pysindy import SINDy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pysindy.optimizers import STLSQ\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "BFytgP62-4Yi",
        "outputId": "a7f551c0-b56b-460b-96a2-6ecf7a4102bd"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"processed_well_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huCGnlkgLqLW",
        "outputId": "bd6e6575-1ff3-4c6b-c13a-3afc0b788201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Well no                    0\n",
            "Dia                        0\n",
            "Dev(deg)                   0\n",
            "LiquidFlowrate             0\n",
            "Gasflowrate                0\n",
            "Area (m2)                  0\n",
            "z                          0\n",
            "GasDens                    0\n",
            "Condesnate Presence        0\n",
            "Water presence             0\n",
            "LiquidDens                 0\n",
            "GasVis                     0\n",
            "LiqVis                     0\n",
            "g (m/s2)                   0\n",
            "Test Vg                    0\n",
            "P/T                        0\n",
            "Test status                0\n",
            "Vsg                        0\n",
            "Vsl                        0\n",
            "Rel                        0\n",
            "Reg                        0\n",
            "film thickness             0\n",
            "d(0,90)                    0\n",
            "d(15,90)                   0\n",
            "d(30,90)                   0\n",
            "d(45,90)                   0\n",
            "d(60,90)                   0\n",
            "d(90,90)                   0\n",
            "d(120,90)                  0\n",
            "d(150,90)                  0\n",
            "d(180,90)                  0\n",
            "d(210,90)                  0\n",
            "d(270,90)                  0\n",
            "d(360,90)                  0\n",
            "friction_factor            0\n",
            "critical_film_thickness    0\n",
            "Qcr                        0\n",
            "Î”Q                         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preparation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split features\n",
        "X = df[['Dia', 'Dev(deg)','Area (m2)', 'z','GasDens','LiquidDens', 'P/T','friction_factor', 'critical_film_thickness']]\n",
        "y = df['Qcr']\n",
        "gsflow = df['Gasflowrate']  # This is your additional target for classification metrics\n",
        "\n",
        "# load class labels: loaded/unloaded/near loaded\n",
        "loading_class = df['Test status'].apply(\n",
        "    lambda x: -1 if x == 'Unloaded' else (0 if x == 'Near L.U' else 1)).to_numpy()\n",
        "\n",
        "# Perform the train-test split, making sure to split all targets simultaneously\n",
        "X_train, X_test, y_train, y_test, gsflow_train, gsflow_test, loading_train, loading_test = train_test_split(\n",
        "    X, y, gsflow, loading_class, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale your features and continuous target (Qcr)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "X_scaled = scaler_X.transform(X)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# t_train is just an index array for plotting\n",
        "t_train = np.arange(len(y_train_scaled))\n",
        "\n",
        "#convert to a numpy array and store test data \n",
        "loading_test = np.array(loading_test)\n",
        "gsflow_test = np.array(gsflow_test)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# accuracy metric and confusion matrix \n",
        "def calculate_accuracy(y_pred, gsflow, loading_actual, interval):\n",
        "    loading_pred = np.where(y_pred > gsflow + interval, 1, \n",
        "                          np.where(y_pred < gsflow - interval, -1, 0))\n",
        "    return accuracy_score(loading_actual, loading_pred), confusion_matrix(loading_actual, loading_pred,  labels=[-1, 0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-fold cross validation\n",
        "def evaluate_sindy(params, X, y, gsflow, loading_class, cv_splits=5):\n",
        "    alpha, threshold, interval = params\n",
        "    kf = KFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
        "    acc_scores = []\n",
        "    \n",
        "    for train_idx, val_idx in kf.split(X):  # Using validation split from training data\n",
        "        # divide into trianing/validation sets\n",
        "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
        "        y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
        "        gsflow_val_cv = gsflow[val_idx]\n",
        "        loading_val_cv = loading_class[val_idx]\n",
        "        \n",
        "        scaler_X = StandardScaler()\n",
        "        X_train_cv_scaled = scaler_X.fit_transform(X_train_cv)\n",
        "        X_val_cv_scaled = scaler_X.transform(X_val_cv)\n",
        "        \n",
        "        scaler_y = StandardScaler()\n",
        "        y_train_cv_scaled = scaler_y.fit_transform(y_train_cv.reshape(-1, 1))\n",
        "        \n",
        "        # Define optimizer for SINDy\n",
        "        optimizer = STLSQ(\n",
        "            alpha=alpha,\n",
        "            threshold=threshold,\n",
        "            max_iter=10000,\n",
        "            normalize_columns=True\n",
        "        )\n",
        "        \n",
        "        # Train model\n",
        "        model = SINDy(optimizer=optimizer)\n",
        "        model.fit(X_train_cv_scaled, t=np.arange(len(y_train_cv_scaled)), \n",
        "                 x_dot=y_train_cv_scaled)\n",
        "        \n",
        "        # Compute performance on validation set \n",
        "        y_val_pred_scaled = model.predict(X_val_cv_scaled)\n",
        "        y_val_pred_cv = scaler_y.inverse_transform(y_val_pred_scaled).flatten()\n",
        "        \n",
        "        acc, cm = calculate_accuracy(y_val_pred_cv, gsflow_val_cv, loading_val_cv, interval)\n",
        "        acc_scores.append(acc)\n",
        "    \n",
        "    return np.mean(acc_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def optimize_sindy_hyperparameters(X_train, y_train, gsflow_train, loading_train, param_grid):\n",
        "    best_score = -1\n",
        "    best_params = None\n",
        "    \n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train).flatten()\n",
        "    gsflow_train = np.array(gsflow_train)\n",
        "    loading_train = np.array(loading_train)\n",
        "    \n",
        "    print(\"Starting hyperparameter optimization...\")\n",
        "    for alpha in param_grid['alpha']:\n",
        "        for threshold in param_grid['threshold']:\n",
        "            for interval in param_grid['interval']:\n",
        "                score = evaluate_sindy((alpha, threshold, interval), \n",
        "                                     X_train, y_train, gsflow_train, loading_train)\n",
        "                \n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {'alpha': alpha, 'threshold': threshold, 'interval': interval}\n",
        "    \n",
        "    return best_params, best_score\n",
        "\n",
        "# Grid search for optimal hyperparameters \n",
        "param_grid = {\n",
        "    'alpha': np.logspace(-4, 0, 10),      \n",
        "    'threshold': np.logspace(-4, 0.25, 10),  \n",
        "    'interval': np.linspace(0, 10, 10)  \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting hyperparameter optimization...\n",
            "\n",
            "Best parameters found:\n",
            "{'alpha': 0.016681005372000592, 'threshold': 0.06812920690579609, 'interval': 0.0}\n",
            "Best CV accuracy: 68.06%\n",
            "\n",
            "=== Final Model Performance ===\n",
            "(x0)' = -375093273090.210 1 + -109.192 x0 + 417301475450.859 x1 + -0.411 x2 + 5.974 x3 + 6.399 x4 + 401432552579.569 x5 + 1.324 x6 + -171.110 x7 + 121.712 x8 + -1.688 x0^2 + -3.595 x0 x1 + -0.367 x0 x2 + 1.015 x0 x3 + -1.721 x0 x4 + -3.586 x0 x5 + -1.211 x0 x6 + -88.999 x0 x7 + 2.397 x0 x8 + 1.094 x1^2 + 1.007 x1 x2 + -6.327 x1 x3 + 3.719 x1 x4 + -446604640816.048 x1 x5 + -0.483 x1 x6 + -4.796 x1 x7 + 2.595 x1 x8 + 2.622 x2^2 + -0.469 x2 x3 + 5.206 x2 x4 + 0.410 x2 x5 + 1.353 x2 x6 + 7.070 x2 x7 + -3.014 x2 x8 + -0.075 x3^2 + -7.505 x3 x4 + 0.639 x3 x5 + 1.038 x3 x6 + 1.961 x3 x7 + -1.030 x3 x8 + -0.106 x4^2 + 5.185 x4 x5 + 3.001 x4 x6 + -9.013 x4 x7 + 18.816 x4 x8 + 1.085 x5^2 + -1.147 x5 x6 + -4.170 x5 x7 + 1.433 x5 x8 + -0.269 x6^2 + 0.346 x6 x7 + 0.426 x7^2 + -0.703 x7 x8 + 0.459 x8^2\n",
            "\n",
            "Training Set Classification Accuracy: 75.30%\n",
            "Test Set Classification Accuracy: 78.57%\n",
            "Confusion Matrix for Test Set Classification\n",
            "[[18  0  6]\n",
            " [ 1  0  0]\n",
            " [ 2  0 15]]\n"
          ]
        }
      ],
      "source": [
        "# Run optimization using ONLY training data\n",
        "best_params, best_score = optimize_sindy_hyperparameters(\n",
        "    X_train, y_train, gsflow_train, loading_train, param_grid\n",
        ")\n",
        "\n",
        "print(\"\\nBest parameters found:\")\n",
        "print(best_params)\n",
        "print(f\"Best CV accuracy: {best_score*100:.2f}%\")\n",
        "\n",
        "# Train final model on FULL training set with best params and evaluate on the final test set\n",
        "final_optimizer = STLSQ(\n",
        "    alpha=best_params['alpha'],\n",
        "    threshold=best_params['threshold'],\n",
        "    max_iter=10000,\n",
        "    normalize_columns=True\n",
        ")\n",
        "\n",
        "final_model = SINDy(optimizer=final_optimizer)\n",
        "final_model.fit(X_train_scaled, t=t_train, x_dot=y_train_scaled)\n",
        "\n",
        "# Evaluate on trainin set\n",
        "y_pred_train_scaled = final_model.predict(X_train_scaled)\n",
        "y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled).flatten()\n",
        "\n",
        "# Evaluate on TEST set (previously unseen data)\n",
        "y_pred_test_scaled = final_model.predict(X_test_scaled)\n",
        "y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled).flatten()\n",
        "\n",
        "# Calculate metrics on TEST set\n",
        "train_acc, train_cm = calculate_accuracy(y_pred_train, gsflow_train, loading_train, best_params['interval'])\n",
        "test_acc, test_cm = calculate_accuracy(y_pred_test, gsflow_test, loading_test, best_params['interval'])\n",
        "\n",
        "print(\"\\n=== Final Model Performance ===\")\n",
        "final_model.print()\n",
        "print(f\"\\nTraining Set Classification Accuracy: {train_acc*100:.2f}%\")\n",
        "print(f\"Test Set Classification Accuracy: {test_acc*100:.2f}%\")\n",
        "print(\"Confusion Matrix for Test Set Classification\")\n",
        "print(test_cm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
