{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1EWx13AGiaPq"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "This version of Feyn and the QLattice is available for academic, personal, and non-commercial use. By using the community version of this software you agree to the terms and conditions which can be found at https://abzu.ai/eula."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pysindy import SINDy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pysindy.optimizers import STLSQ\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from pysindy.feature_library import PolynomialLibrary, FourierLibrary, GeneralizedLibrary\n",
        "\n",
        "# Model agnostic \n",
        "from typing import Optional, List, Callable, Dict, Any, List\n",
        "from pathlib import Path\n",
        "from itertools import islice\n",
        "from  utils import ChiefBldr  # custom model for data handling/model trianing\n",
        "\n",
        "# Model specific \n",
        "from typing import Optional, List "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the directory this file lives in\n",
        "nb_dir = Path.cwd() # notebook directory\n",
        "project_root = nb_dir.parents[1] # project directory\n",
        "data_path = project_root / \"datasets\" / \"processed_well_data.csv\"\n",
        "\n",
        "includ_cols = ['Dia', 'Dev(deg)','Area (m2)', 'z','GasDens','LiquidDens', 'P/T','friction_factor', 'critical_film_thickness']\n",
        "D = ChiefBldr(path=data_path, includ_cols=includ_cols, test_size=0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model and optimizing hyperparameters via k-fold CV...\n",
            "Done. Best score = 0.7463458110516934\n",
            "Best hyperparameters: {'alpha': 1.7782794100389228, 'threshold': 0.1, 'degree': 2, 'n_frequencies': 2}\n",
            "Retraining optimized model on full training set\n",
            "Training set score: 0.7650602409638554\n",
            "Test set score: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "def sindy(\n",
        "        hparams: Dict[str,Any]\n",
        "):      \n",
        "        # partition dict by method\n",
        "        hparams_opt = dict(list(hparams.items())[:2])\n",
        "        hparams_poly = dict(list(hparams.items())[-2:-1])\n",
        "        hparams_fourier = dict(list(hparams.items())[-1:])\n",
        "        \n",
        "        # Define optimizer for SINDy\n",
        "        hparams_opt = dict(islice(hparams.items(), 2))\n",
        "        optimizer = STLSQ(\n",
        "        max_iter=10000,\n",
        "        normalize_columns=True,\n",
        "        **hparams_opt,\n",
        "        )\n",
        "        # specify feature lib\n",
        "        poly_library = PolynomialLibrary(**hparams_poly)\n",
        "        fourier_library = FourierLibrary(**hparams_fourier)\n",
        "        lib = GeneralizedLibrary([poly_library, fourier_library])\n",
        "        model = SINDy(optimizer=optimizer, feature_library=lib)\n",
        "\n",
        "        return model \n",
        "\n",
        "hparam_grid = {\n",
        "    'alpha': np.logspace(-4, 0.25, 10),      \n",
        "    'threshold': np.logspace(-4, -1, 10),  \n",
        "    'degree': [1, 2, 3, 5],\n",
        "    'n_frequencies': [1, 2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# train model and optimize hyperparameters via grid search \n",
        "trained_model = D.evolv_model(build_model=sindy, hparam_grid=hparam_grid, k_folds=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preparation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-fold cross validation\n",
        "\n",
        "def evaluate_sindy(params, X, y, gsflow, loading_class, cv_splits=5):\n",
        "    alpha, threshold, interval, n, f = params\n",
        "    \n",
        "    # Use StratifiedKFold to preserve class balance in splits\n",
        "    kf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    acc_scores = []\n",
        "    \n",
        "    for train_idx, val_idx in kf.split(X, loading_class):  # Using validation split from training data\n",
        "        # divide into trianing/validation sets\n",
        "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
        "        y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
        "        gsflow_val_cv = gsflow[val_idx]\n",
        "        loading_val_cv = loading_class[val_idx]\n",
        "        \n",
        "        scaler_X = StandardScaler()\n",
        "        X_train_cv_scaled = scaler_X.fit_transform(X_train_cv)\n",
        "        X_val_cv_scaled = scaler_X.transform(X_val_cv)\n",
        "        \n",
        "        scaler_y = StandardScaler()\n",
        "        y_train_cv_scaled = scaler_y.fit_transform(y_train_cv.reshape(-1, 1))\n",
        "        \n",
        "        # Define optimizer for SINDy\n",
        "        optimizer = STLSQ(\n",
        "            alpha=alpha,\n",
        "            threshold=threshold,\n",
        "            max_iter=10000,\n",
        "            normalize_columns=True\n",
        "        )\n",
        "        \n",
        "        model.fit(X_train_cv_scaled, t=np.arange(len(y_train_cv_scaled)), \n",
        "                 x_dot=y_train_cv_scaled)\n",
        "        \n",
        "        # Compute performance on validation set \n",
        "        y_val_pred_scaled = model.predict(X_val_cv_scaled)\n",
        "        y_val_pred_cv = scaler_y.inverse_transform(y_val_pred_scaled).flatten()\n",
        "        \n",
        "        acc, cm = calculate_accuracy(y_val_pred_cv, gsflow_val_cv, loading_val_cv, interval)\n",
        "        acc_scores.append(acc)\n",
        "    \n",
        "    return np.mean(acc_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def optimize_sindy_hyperparameters(X_train, y_train, gsflow_train, loading_train, param_grid):\n",
        "    best_score = -1\n",
        "    best_params = None\n",
        "    \n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train).flatten()\n",
        "    gsflow_train = np.array(gsflow_train)\n",
        "    loading_train = np.array(loading_train)\n",
        "    \n",
        "    print(\"Begin training and hyperparameter optimization...\")\n",
        "    for alpha in param_grid['alpha']:\n",
        "        for threshold in param_grid['threshold']:\n",
        "            for interval in param_grid['interval']:\n",
        "                for n in param_grid['n']:\n",
        "                    for f in param_grid['f']:\n",
        "                        score = evaluate_sindy((alpha, threshold, interval, n, f), \n",
        "                                            X_train, y_train, gsflow_train, loading_train)\n",
        "                    \n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_params = {'alpha': alpha, 'threshold': threshold, 'interval': interval, 'n': n}\n",
        "    \n",
        "    return best_params, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid search for optimal hyperparameters \n",
        "param_grid = {\n",
        "    'alpha': np.logspace(-4, 0.25, 10),      \n",
        "    'threshold': np.logspace(-4, -1, 10),  \n",
        "    'interval': np.linspace(0, 10, 10) , \n",
        "    'n': np.array([1, 2, 3, 5]),\n",
        "    'f':np.array([1, 2, 3, 4, 5])\n",
        "}\n",
        "\n",
        "#  Train model and optimize hyperparameters \n",
        "best_params, best_score = optimize_sindy_hyperparameters(\n",
        "    X_train, y_train, gsflow_train, loading_train, param_grid\n",
        ")\n",
        "\n",
        "print(\"\\nBest parameters found:\")\n",
        "print(best_params)\n",
        "print(f\"Best CV accuracy: {best_score*100:.2f}%\")\n",
        "\n",
        "# Train final model on FULL training set with best params and evaluate on the final test set\n",
        "final_optimizer = STLSQ(\n",
        "    alpha=best_params['alpha'],\n",
        "    threshold=best_params['threshold'],\n",
        "    max_iter=10000,\n",
        "    normalize_columns=True\n",
        ")\n",
        "\n",
        "# Evaluate model performance \n",
        "fourier_library = FourierLibrary(n_frequencies=int(best_params['n']))\n",
        "poly_library = PolynomialLibrary(degree=int(best_params['n']))\n",
        "lib = GeneralizedLibrary([poly_library, fourier_library])\n",
        "\n",
        "final_model = SINDy(optimizer=final_optimizer, feature_library=lib)\n",
        "final_model.fit(X_train_scaled, t=t_train, x_dot=y_train_scaled)\n",
        "\n",
        "# Evaluate on trainin set\n",
        "y_pred_train_scaled = final_model.predict(X_train_scaled)\n",
        "y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled).flatten()\n",
        "\n",
        "# Evaluate on TEST set (previously unseen data)\n",
        "y_pred_test_scaled = final_model.predict(X_test_scaled)\n",
        "y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled).flatten()\n",
        "\n",
        "# Calculate metrics on TEST set\n",
        "train_acc, train_cm = calculate_accuracy(y_pred_train, gsflow_train, loading_train, best_params['interval'])\n",
        "test_acc, test_cm = calculate_accuracy(y_pred_test, gsflow_test, loading_test, best_params['interval'])\n",
        "\n",
        "print(\"\\n=== Final Model Performance ===\")\n",
        "final_model.print()\n",
        "print(f\"\\nTraining Set Classification Accuracy: {train_acc*100:.2f}%\")\n",
        "print(f\"Test Set Classification Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "print(\"Confusion Matrix for Train Set Classification\")\n",
        "print(train_cm)\n",
        "print(\"Confusion Matrix for Test Set Classification\")\n",
        "print(test_cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification graph for well status "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set publication-friendly plotting settings\n",
        "plt.rcParams['text.usetex'] = False\n",
        "plt.rcParams['font.family'] = 'Arial'  # Publication-friendly font\n",
        "plt.rcParams['axes.linewidth'] = 0.8\n",
        "plt.rcParams['xtick.direction'] = 'in'\n",
        "plt.rcParams['ytick.direction'] = 'in'\n",
        "\n",
        "# Define a light, publication-friendly color palette without green\n",
        "color_map = {\n",
        "    'Loaded': '#FF6363',  # Light red\n",
        "    'Unloaded': '#3674B5',  # Light purple\n",
        "    'Questionable': '#D3D3D3',  # Light orange\n",
        "    'Near L.U': '#FFCC99'  # Light blue\n",
        "}\n",
        "status_col = 'Test status'\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "df.rename(columns={\n",
        "    'Dev(deg)': 'Dev_deg',\n",
        "    'Area (m2)': 'Area_m2',\n",
        "    'g (m/s2)': 'g_m_s2',\n",
        "    'P/T': 'P_T'\n",
        "}, inplace=True)\n",
        "\n",
        "# Map Test status to numerical classes\n",
        "loading_class_map = {'Unloaded': -1, 'Near L.U': 0, 'Loaded': 1, 'Questionable': 1}\n",
        "df['loading_class'] = df[status_col].map(loading_class_map)\n",
        "\n",
        "# Define features and targets\n",
        "features = ['Dia', 'Dev_deg', 'Area_m2', 'z', 'GasDens', 'LiquidDens', 'g_m_s2', 'P_T', 'friction_factor', 'critical_film_thickness']\n",
        "output = 'Qcr'\n",
        "gasflow = 'Gasflowrate'\n",
        "status_col = 'Test status'\n",
        "\n",
        "X = df[features]\n",
        "y = df[output]\n",
        "gsflow = df[gasflow]\n",
        "loading_class = df['loading_class']\n",
        "X_train, X_test, y_train, y_test, gsflow_train, gsflow_test, loading_train, loading_test = train_test_split(\n",
        "    X, y, gsflow, loading_class, test_size=0.2, random_state=42, stratify=loading_class\n",
        ")\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "X_scaled = scaler_X.transform(X)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n",
        "y_scaled = scaler_y.transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Slice the first 40 entries for plotting.\n",
        "gasflow_subset = gsflow\n",
        "y_pred_subset = trained_model.predict(X_scaled)\n",
        "colors = df[status_col].map(color_map).fillna('#D3D3D3')  # Light gray for missing values\n",
        "\n",
        "plt.figure(figsize=(8, 6), dpi=300)\n",
        "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.3, font='Arial')\n",
        "plt.scatter(gasflow_subset, y_pred_subset, c=colors, alpha=1, s=100, edgecolors=\"gray\", linewidth=0.5)\n",
        "plt.plot([0, 350000], [0, 350000], '--', color='#FF6666', linewidth=1.5)  # Light red dashed line\n",
        "plt.title(\"PySINDy Model\", fontsize=16, fontweight='bold', pad=15)\n",
        "plt.xlabel(\"Measured Well Flow Rate (m³/day)\", fontsize=14, fontweight='bold')\n",
        "plt.ylabel(\"Predicted Critical Rate (m³/day)\", fontsize=14, fontweight='bold')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xlim(0, 350000)\n",
        "plt.ylim(0, 350000)\n",
        "legend_patches = [mpatches.Patch(color=color, label=status) for status, color in color_map.items()]\n",
        "plt.legend(handles=legend_patches, title='Actual Label', fontsize=12, title_fontsize=14,\n",
        "           loc='best', frameon=True, edgecolor='gray')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"pysindy_scatter_new.pdf\", format=\"pdf\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fit model for literature comparsion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split features\n",
        "X = df[['Dia', 'Dev(deg)','Area (m2)', 'z','GasDens','LiquidDens', 'P/T','friction_factor', 'critical_film_thickness']]\n",
        "y = df['Qcr']\n",
        "gsflow = df['Gasflowrate']  # This is your additional target for classification metrics\n",
        "\n",
        "# load class labels: loaded/unloaded/near loaded\n",
        "loading_class = df['Test status'].apply(\n",
        "    lambda x: -1 if x == 'Unloaded' else (0 if x == 'Near L.U' else 1)).to_numpy()\n",
        "\n",
        "# Perform the train-test split, making sure to split all targets simultaneously\n",
        "X_train, X_test, y_train, y_test, gsflow_train, gsflow_test, loading_train, loading_test = train_test_split(\n",
        "    X, y, gsflow, loading_class, test_size=0.20, random_state=42, stratify=loading_class\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "For final evaluation of the model, we need designate test data and normalize to all training data\n",
        "\"\"\"\n",
        "# Scale your features and continuous target (Qcr)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "X_scaled = scaler_X.transform(X)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "y_scaled = scaler_y.transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# t_train is just an index array for plotting\n",
        "t_train = np.arange(len(y_train_scaled))\n",
        "t = np.arange(len(y_scaled))\n",
        "\n",
        "#convert to a numpy array and store test data \n",
        "loading_train = np.array(loading_train)\n",
        "loading_test = np.array(loading_test)\n",
        "loading = np.array(loading_class)\n",
        "gsflow_test = np.array(gsflow_test)\n",
        "gsflow = np.array(gsflow)\n",
        "y_test = np.array(y_test)\n",
        "y = np.array(y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
